
In July 1996, Sakai City, Japan, experienced the largest outbreak of Escherichia coli 0157:H7 infections ever reported, involving over 7,000 persons. Michino et al. (1) have convincingly demonstrated through a review of school absentee records, a cohort study of over 47,000 schoolchildren, product traceback, and molecular subtyping that illness was due to consumption of contaminated white radish sprouts served through a centralized lunch program. Multiple other outbreaks of E. coli 0157:H7 infections occurred in Japan during the same summer (2). Investigations of these outbreaks as well as the one in Sakai City highlight some of the problems that face public health officials worldwide and illustrate lessons to be learned for investigating foodborne disease outbreaks.
The authors conducted a case-control study among premenopausal women in the Baltimore, Maryland, area to examine the associations of uterine leiomyoma with ethnicity and hormonerelated characteristics. Cases of uterine leiomyoma (n = 318) were surgically or sonographically first confirmed between January 1990 and June 1993. A total of 394 controls were selected from women who were visiting their gynecologist for a routine checkup. Data were collected through telephone interviews and abstraction of medical records; 77.8% of eligible cases and 78.0% of Aeligible controls were interviewed. Positive adjusted associations were observed between risk of uterine leiomyoma and self-described African-American ethnicity (vs. Whites: odds ratio (OR) = 9.4; 95% confidence interval (CI): 5.7, 15.7), early menarche (<11 years vs. >13 years: OR = 2.4; 95% CI: 1.1, 5.6), and high body mass index (upper quartile vs. lower quartile: OR = 2.3; 95% CI: 1.4, 3.8). Inverse associations were observed with use of oral contraceptives (current use vs. never use: OR = 0.2, 95% CI: 0.1, 0.6) and duration of smoking (=" border=0 src="/math/ge.gif"19 years vs. never: OR = 0.6; 95% CI: 0.4, 1.1). Younger ages at infertility diagnosis and at first and last childbirth were more common among cases; however, analyses of data on tumor location suggested that these associations represent predominantly consequences of uterine leiomyoma. These results suggest that development of uterine leiomyoma is associated with increased exposure to ovarian hormones. Possible reasons for the very elevated risk among African-American women need further investigation.
In this case-control study, the authors analyzed associations of uterine leiomyoma with atherogenic risk factors and potential sources of uterine irritation. The study included 318 case women with uterine leiomyoma that was first confirmed between 1990 and 1993 in the Baltimore, Maryland, area and 394 controls selected from women visiting the same gynecologists' offices for routine reasons. Telephone interviews were conducted with 77.8% of eligible cases and 78.0% of eligible controls. Compared with participants with no hypertension history, increased risks were observed among participants with any history of hypertension (odds ratio (OR) = 1.7; 95% confidence interval (CI): 1.0, 2.8), hypertension requiring medication (OR = 2.1; 95% CI: 1.1, 4.1), hypertension diagnosed at ages less than 35 years (for hypertension requiring medication, OR = 2.7; 95% CI: 1.0, 7.6), and hypertension of 5 or more years' duration (for hypertension requiring medication, OR = 3.1; 95% CI: 1.2, 8.2). Estimates of associations with diabetes history were very imprecise but followed similar patterns. Adjusted associations were observed with pelvic inflammatory disease (three or more episodes vs. none: OR = 3.7; 95% CI: 0.9, 15.9), chlamydial infection (history vs. no history: OR = 3.2; 95% CI: 0.8, 13.7), and use of an intrauterine device when it caused infectious complications (use vs. no use: OR = 5.3; 95% CI: 1.8, 16.3). Risk of uterine leiomyoma was also associated in a graded fashion with frequency of perineal talc use (daily use vs. no use: OR = 2.2; 95% CI: 1.4, 3.1). The authors conclude that nonhormonal factors may influence risk of uterine leiomyoma.
Uterine leiomyomas are reported to be the most common benign gynecologic tumors affecting premenopausal women, and they are often associated with considerable morbidity. The purpose of this study was to identify risk factors for uterine fibroids among women undergoing tubal sterilization. Cases comprised women aged 17–44 years whose uterine fibroids were first visualized at the time of tubal sterilization (1978–1979 or 1985–1987) or who reported a history of uterine fibroids (n = 317). Controls were randomly selected from women with no laparoscopic evidence of or history of fibroids (n = 1,268). Adjusted odds ratios were estimated using unconditional logistic regression separately for White (n = 1,235) and African-American(n = 350) women. Risk factors for White women included: age 40–44 years (odds ratio (OR) = 6.3; 95% confidence interval (CI): 3.5, 11.6), =" border=0 src="/math/ge.gif"5 years since last delivery (OR = 1.9; 95% CI: 1.1, 3.1), lifetime cigarette smoking of =" border=0 src="/math/ge.gif"1 pack/day (OR = 1.6; 95% CI: 1.1, 2.3), menstrual cycle length of >30 days (OR = 1.6; 95% CI: 1.1, 3.3), and menstrual bleeding for =" border=0 src="/math/ge.gif"6 days (OR = 1.4; 95% CI: 1.0, 2.0). Parous women were at reduced risk compared with nulliparous women (OR = 0.2; 95% CI: 0.1, 0.3). Advancing age was the only significant risk factor for African-American women (ages 40–44 years, OR = 27.5; 95% CI: 5.6, 83.6). Current oral contraceptive use and elective abortion were not associated with fibroids.
Uterine leiomyomata have a substantial impact on women's reproductive health, but epidemiologists have focused relatively little energy on identifying risk factors for this condition. Only a handful of studies, most of which were not designed to address methodological challenges posed by these tumors, have been conducted. These studies focused almost exclusively on reproductive and hormonal characteristics as possible risk factors, but consistent relations have not emerged. Three new reports (from two studies) in this issue of the Journal target the paucity of information on uterine leiomyomata risk factors by testing novel hypotheses, by employing designs that incorporate Asubclinical tumors or account for variable management of clinically recognized disease, or by using a combination of these approaches. The success of these strategies and the contributions of Athe new findings are discussed. Recommendations are made for a program of research that eventually could improve our knowledge of uterine leiomyomata etiology and yield clues to the prevention of associated morbidity.
Although postmenopausal estrogen use has been associated with a lower risk of colon cancer in women, some studies do not confirm such findings. No known study has examined the effect of Acumulative estrogen exposure on colon cancer risk. Bone mass has been proposed as a marker Aof cumulative exposure to endogenous and exogenous estrogens. By using data on 1,394 Massachusetts women in the Framingham Study who underwent hand radiography in 1967–1970, the authors examined the association between bone mass (from relative areas of the second metacarpal) and colon cancer incidence. Over 27 years of follow-up, 44 incident colon cancer cases occurred. Colon cancer incidence decreased from 2.19 per 1,000 person-years among the women in the lowest age-specific tertile of bone mass to 1.59 and 1.08 among women in the middle and the highest tertiles, respectively. After adjustment for age and other potential confounding factors, the rate ratios of colon cancer were 1.0, 0.7 (95% confidence interval: 0.3, 1.3), and 0.4 (95% confidence interval: 0.2, 0.9) from the lowest to the highest tertile (p for trend = 0.033). No association was found between bone mass and rectal cancer. The findings suggest that women with higher bone mass, perhaps reflecting greater cumulative estrogen exposure, have a decreased risk of colon cancer.
Although several epidemiologic studies have been conducted on alcohol consumption and bladder cancer risk, the risk according to quantity and type of alcohol consumed is not clear. The authors investigated these associations in a large prospective cohort study on diet and cancer among 120,852 subjects in the Netherlands aged 55–69 years at baseline (1986). Subjects completed a questionnaire on risk factors for cancer, including alcohol consumption. Follow-up for incident cancer was established by record linkage to cancer registries. The case-cohort analysis was restricted to a follow-up period of 6.3 years and was based on 594 cases with bladder cancer and 3,170 subcohort members. The authors corrected for age and smoking in multivariable analyses. The incidence rate ratios for men who consumed <5, 5–<15, 15–<30, and =" border=0 src="/math/ge.gif"30 grams of alcohol per day were 1.49, 1.52, 1.16, and 1.63 compared with nondrinkers, respectively (p for trend = 0.13). Alcohol consumed from beer, wine, and liquor was associated with moderately elevated risks, although most were not statistically significant. The incidence rate ratios for women varied around unity. The results of this study do not suggest an important association between alcohol consumption and bladder cancer risk.
In a 1990–1996 case-control study in western Germany, the authors investigated lung cancer risk due to exposure to residential radon. Confirmed lung cancer cases from hospitals and a random sample of community controls were interviewed by trained interviewers regarding different risk factors. For 1 year, alpha track detectors were placed in dwellings to measure radon gas Aconcentrations. The evaluation included 1,449 cases and 2,297 controls recruited from the entire Astudy area and a subsample of 365 cases and 595 controls from radon-prone areas of the basic study region. Rate ratios were estimated by using conditional logistic regression adjusted for smoking and for asbestos exposure. In the entire study area, no rate ratios different from 1.0 were found; in the radon-prone areas, the adjusted rate ratios for exposure in the present dwelling were 1.59 (95% confidence interval (CI): 1.08, 2.27), 1.93 (95% CI: 1.19, 3.13), and 1.93 (95% CI: 0.99, 3.77) for 50–80, 80–140, and >140 Bq/m3, respectively, compared with 0–50 Bq/m3. The excess rate ratio for an increase of 100 Bq/m3 was 0.13 (-0.12 to 0.46). An analysis based on cumulative exposure produced similar results. The results provide additional evidence that residential radon is a risk factor for lung cancer, although a risk was detected in radon-prone areas only, not in the entire study area.
For determination of whether plasma 1,1-dichloro-2,2-bis(p-chlorophenyl)ethylene (DDE) pesticide levels (<=1–32 ppb) are associated with immune suppression or DNA damage in lymphocytes, 302 individuals residing in Moore County, North Carolina, in 1994–1996 provided a blood specimen, underwent a skin test, and answered a questionnaire concerning factors affecting plasma organochlorine pesticide levels and the immune system. The blood specimens were analyzed for levels of plasma DDE (a metabolite of 1,1,1-trichloro-2,2-bis(p-chlorophenyl)ethane), numbers and types of blood cells, immunoglobulin levels, mitogen-induced lymphoproliferative activity, and lymphocyte micronuclei. When DDE levels were categorized as 1 or less, more than 1 to 2, more than 2 to 4.3, more than 4.3 to 7.6, and more than 7.6 ppb, individuals with higher plasma DDE levels had lowered mitogen-induced lymphoproliferative activity (concanavalin A, range: 74,218 dropping to 55,880 counts per minute, p = 0.03) and modestly increased total lymphocytes (range: 2.0–2.3 x 103/µl, p = 0.05) and immunoglobulin A levels (range: 210–252 mg/dl, p = 0.04). There were no consistent differences in response to the skin tests by plasma DDE levels. Plasma DDE levels were not associated with a higher frequency of micronuclei. The authors conclude that relatively low levels of plasma DDE are associated with statistically significant changes in immune markers, although the magnitude of the effects are of uncertain clinical importance.
The objective of this study was to investigate the effects of an average volume of alcohol consumption and drinking patterns on all-cause mortality. The sample (n = 5,072) was drawn from the 1984 National Alcohol Survey, representative of the US population living in households. Follow-up time was until the end of 1995, with 532 people deceased during this period. The authors found a significant influence of drinking alcohol on mortality with a J-shaped association for males and an insignificant relation of the same shape for females. When the largest categories of equivalent average volume of consumption were divided into people with and without heavy drinking occasions, serving as an indicator of drinking pattern, this differentiation proved important in predicting mortality. Light to moderate drinkers had higher mortality risks when they reported heavy drinking occasions (defined by either eight drinks per occasion or getting drunk at least monthly). Similarly, when the category of exdrinkers was divided into people who did or did not report heavy drinking occasions in the past, people with heavy drinking occasions had a higher mortality risk. Finally, indicating alcohol problems in the past was related to higher mortality risk. Results emphasized the importance of routinely including measures of drinking patterns into future epidemiologic studies on alcohol-related mortality.
In the Systolic Hypertension in the Elderly Program (SHEP) trial (1985–1990), active treatment reduced the incidence of cardiovascular events, but not that of dementia and disability, as compared with placebo. This study aims to evaluate if assessment of cognitive and functional outcomes was biased by differential dropout. Characteristics of subjects who did or did not participate in follow-up cognitive and functional evaluations were compared. The relative risks of incident cognitive impairment and disability were assessed in the two treatment groups, with the use of the reported findings and under the assumption that the proportions of cognitive and functional impairment among dropouts increased. Assignment to the placebo group and the occurrence of cardiovascular events independently predicted missed assessments. From the reported findings, the risk of cognitive and functional impairment was similar between the two treatment groups. However, when 20–30% and 40–80% of the subjects who missed the assessment were assumed to be cognitively and, respectively, functionally impaired, assignment to active treatment reduced the risk of these outcomes. In the SHEP, the cognitive and functional evaluations were biased toward the null effect by differential dropout. This might have obscured the appraisal of a protective effect of treatment on the cognitive and functional decline of older hypertensive adults.
The adrenal steroid dehydroepiandrosterone (DHEA) and its sulfate (DHEAS) have been characterized as "protective" against ischemic heart disease (IHD), especially in men, on the basis of sparse epidemiologic evidence. The authors used data from the Massachusetts Male Aging Study, a random sample prospective study of 1,709 men aged 40–70 years at baseline, to test whether serum levels of DHEA or DHEAS could predict incident IHD over a 9-year interval. At baseline (1987–1989) and follow-up (1995–1997), an interviewer-phlebotomist visited each subject in his home to obtain comprehensive health information, body measurements, and blood samples for hormone and lipid analysis. Incident IHD between baseline and follow-up was ascertained from hospital records and death registries, supplemented by self-report and evidence of medication. In the analysis sample of 1,167 men, those with serum DHEAS in the lowest quartile at baseline (<1.6 µg/ml) were significantly more likely to incur IHD by follow-up (adjusted odds ratio = 1.60, 95 percent confidence interval: 1.07, 2.39; p = 0.02), independently of a comprehensive set of known risk factors including age, obesity, diabetes, hypertension, smoking, serum lipids, alcohol intake, and physical activity. Low serum DHEA was similarly predictive. These results confirm prior evidence that low DHEA and DHEAS can predict IHD in men.
The design of a screening program for asymptomatic genital infections with Chlamydia trachomatis, requires decisions about which sex or age group should be targeted and whether partner referral should be included in the program. To investigate the effects of various screening programs on the prevalence and incidence of asymptomatic C. trachomatis infections in women, in May 1996 to April 1997 in Bilthoven, the Netherlands, the authors used a stochastic simulation model for C. trachomatis transmission in an age-structured, heterosexual population with a sexually highly active core group. Different screening scenarios were implemented over a time period of 10 years. Prevalence, incidence, and the fraction of infected persons found by partner referral were computed. Through screening of men and women between ages 15 and 24 years (baseline scenario), the prevalence of asymptomatic infections in women could be reduced from 4.2% to 1.4% in 10 years. Increasing the age range of screening up to ages 29 or 34 years led to prevalences of 0.4% and 0.06%, respectively, after 10 years. About 28% of all infected persons were found via partner referral. There are considerable indirect positive effects of screening on those population groups that are not included in the screening because of the reduced risk of becoming infected. Partner referral contributes substantially to prevalence reduction.
Molecular biology techniques have become increasingly integrated into the practice of infectious disease epidemiology. The term "molecular epidemiology" routinely appears in the titles of articles that use molecular strain-typing ("fingerprinting") techniques—regardless of whether there is any epidemiologic application. What distinguishes molecular epidemiology is both the "molecular," the use of the techniques of molecular biology, and the "epidemiology," Athe study of the distribution and determinants of disease occurrence in human populations. The authors review various definitions of molecular epidemiology. They then comment on the range of molecular techniques available and present some examples of the benefits and challenges of applying these techniques to infectious agents and their affected host using tuberculosis and urinary tract infection as examples. They close with some thoughts about training future epidemiologists to best take advantage of the new opportunities that arise from integrating epidemiologic methods with modern molecular biology.
The consumption of vegetables and fruit may protect against many types of cancer, but research evidence is not compelling for breast cancer. Carotenoids are pigments that are present in most plants and have known antioxidant properties. Blood concentrations of carotenoids have been proposed as integrated biochemical markers of vegetable, fruit, and synthetic supplements consumed. In a case-control study (270 cases, 270 controls) nested within a cohort in New York during 1985–1994, the carotenoids lutein, zeaxanthin, ?cryptoxanthin, lycopene, AL[HA-carotene, and ?-carotene were measured in archived serum samples using liquid chromatography. There was an evident increase in the risk of breast cancer for decreasing ?-carotene, lutein, ALPHA-carotene, and ?-cryptoxanthin. The risk of breast cancer approximately doubled among subjects with blood levels of ?-carotene at the lowest quartile, as compared with those at the highest quartile (odds ratio = 2.21; 95% confidence interval (CI): 1.29, 3.79). The risk associated with the other carotenoids was similar, varying between 2.08 (95% CI: 1.11, 3.90) for lutein and 1.68 (95% CI: 0.99, 2.86) for ?-cryptoxanthin. The odds ratio for the lower quartile of total carotenoids was 2.31 (95% CI: 1.35, 3.96). These observations offer evidence that a low intake of carotenoids, through poor diet and/or lack of vitamin supplementation, may be associated with increased risk of breast cancer and may have public health relevance for people with markedly low intakes.
A population-based case-control study of prostate cancer was performed in King County, Washington, in White men and Black men aged 40–64 years, between 1993 and 1996. Incident prostate cancer cases (n = 753) were identified from the Seattle-Puget Sound Surveillance, Epidemiology, and End Results (SEER) cancer registry. Controls (n = 703) were identified through random digit dialing and were frequency matched to cases on age. Sexual behavior, medical history, and other potential prostate cancer risk factors were ascertained through an in-person interview. There was no relation between sexual orientation and prostate cancer, although the number of men who had sex with men was small. Risk estimates increased directly with the lifetime number of female sexual partners (trend p < 0.001) but not with male partners (trend p = 0.62). Risk also increased with decreasing age at first intercourse, but this effect disappeared after adjusting for the number of female partners. Prior infection with gonorrhea was positively associated with risk (odds ratio = 1.50; 95% confidence interval: 1.0, 2.2), but no effect was seen among men with other sexually transmitted diseases. No relation between lifetime frequency of sexual intercourse and risk of prostate cancer was apparent. These findings are consistent with previous studies that support an infectious etiology for prostate cancer.
The purpose of this cross-sectional analysis of women aged 35–49 years from the Third National Health and Nutrition Examination Survey, conducted between 1988 and 1994, was to assess associations with menopausal status based either on menstrual cycle patterns or on elevated (>20 IU/liter) follicle-stimulating hormone. Menstrual cycle-based menopausal status was defined for women who had not had surgical menopause by months since the last period (<2, 2–12, and >12 months for pre-, peri-, and postmenopause, respectively). Logistic regression was adjusted for age, smoking, and unilateral oophorectomy. Higher body mass index (30 kg/m2 compared with < 25.0 kg/m2) was associated with a lower likelihood of elevated follicle-stimulating hormone (odds ratio (OR)=0.6, 95% confidence interval (CI): 0.4, 0.9) but this association was not seen with the menstrual measure of menopause. Exercise (three or more times per week) was associated with a lower likelihood of being postmenopausal on the basis of menstrual (OR = 0.3, 95% CI: 0.2, 0.7) and hormonal (OR = 0.6, 95% CI: 0.4, 1.0) measures. Alcohol use also tended to be associated with postmenopausal status by either measure, but not significantly so. There was little evidence of associations with ethnicity, education, age at menarche, number of livebirths, and oral contraceptive use. Menstrual-based definitions of menopause can be misclassified for women with menstrual irregularity. This might explain why obese women were classified menstrually as menopausal while remaining hormonally premenopausal.
Several studies indicate that parity and lactation are associated with modest, short-term bone loss, but the long-term effect on osteoporotic fracture risk is uncertain. The authors therefore analyzed data from a population-based case-control study among Swedish postmenopausal women aged 50–81 years between October 1993 and February 1995. Mailed questionnaires and telephone interviews were used to collect data on 1,328 incident cases with hip fracture and 3,312 randomly selected controls. In age-adjusted analyses, the risk of hip fracture among all women was reduced by 10% per child (95% confidence interval (CI): 5, 14). After multivariate adjustment including body mass index as a covariate, the risk reduction was 5% per child (95% CI: 0, 10). Oral contraceptive use modified the association of parity with hip fracture risk. Among never users of oral contraceptives, the risk of hip fracture was reduced by 8% per child (95% CI: 2, 13), whereas among ever users of oral contraceptives, the risk was in the opposite direction, with an increase in risk by 19% per child (95% CI: 0, 41). After parity was considered, there was no association of duration of lactation period with fracture risk. The authors conclude that parity is modestly associated with a reduced hip fracture risk among women who had not used oral contraceptives previously.
Variability of angina symptoms over a 5-year period was examined in a prospective study, in which 7,109 British middle-aged men completed two chest pain questionnaires, Q1 (1978–1980) and Q5 (1983–1985), and were classified as having no chest pain, nonexertional chest pain, or angina (Q) (exertional chest pain) on each occasion. Within persons, there was considerable variability in response to the chest pain questions at Q1 and Q5. Angina (Q) persistence showed marked associations with previous myocardial infarction, diagnosed angina, electrocardiogram ischemia, and subsequent major ischemic heart disease events from Q5 onward. Compared with men without angina (Q), the age-adjusted hazard ratios were 1.5 (95% confidence interval (CI): 1.1, 2.2) for angina (Q) at Q1 only, 2.6 (95% CI: 2.1, 3.2) for angina (Q) at Q5 only, and 3.4 (95% CI: 2.8, 4.3) for angina (Q) on both occasions. For men without diagnosed ischemic heart disease, for whom apparent remission of angina (Q) was particularly frequent, a similar pattern of association was found between angina (Q) persistence and subsequent major events. In middle-aged men, exertional chest pain is a strong indicator of major coronary risk but frequently appears transient in the longer term. Persistently reported symptoms are associated with severe disease and a poor prognosis.
The distribution of serum C-reactive protein (CRP) levels and their association with age, sex, and atherosclerotic risk factors were studied in a large Japanese population between 1992 and 1995. The subjects consisted of 2,275 males and 3,832 females aged 30 years and over. CRP was measured by nephelometry. The distribution of CRP was highly skewed toward a lower level than that of previous studies and seemed to be a combination of two separate distribution curves. The increase in CRP with age was statistically significant, and males had higher CRP levels than did females. Males who were current smokers had higher CRP levels than did nonsmokers. Age, systolic blood pressure, diastolic blood pressure, triglycerides, fibrinogen, and body mass index were all positively associated with CRP in both sexes, while total cholesterol and blood glucose were positively related in females only. High density lipoprotein cholesterol was inversely related in both sexes. Multiple logistic regression analysis showed that sex, age, systolic pressure, high density lipoprotein cholesterol, triglycerides, fibrinogen, and body mass index were significant independent variables. In conclusion, the distribution of CRP among the Japanese was quite different from that among Westerners, although CRP levels correlated with other atherosclerotic risk factors, similar to those in Westerners.
The authors compared generational and regional trends of premature mortality from ischemic heart disease (IHD) from 1969 to 1992 for persons aged 30–69 years. They selected Tokyo and Osaka prefectures as the most urbanized and compared them with the rest of Japan. The data were divided into two periods: period I (1969–1978, International Classification of Diseases, Eight Revision) and period II (1979–1992, International Classification of Diseases, Ninth Revision). In both populations, IHD mortality decreased for both sexes, but mortality from nonspecific heart disease remained constant in men and decreased in women. In Tokyo and Osaka prefectures, the percentage decline per year in IHD mortality for both sexes was significantly smaller in period II than in period I. However, in the rest of Japan, it did not decrease for either sex. Age-specific analysis showed that the percentage decline per year in period II was smallest for the group aged 30–49 years (men, 0.05%; women, 0.76%) in Tokyo and Osaka prefectures, while it was similar for all age groups in the rest of Japan. For men, the IHD mortality rate in 1991–1992 for those aged 30–49 years was higher in Tokyo and Osaka prefectures (9.4/100,000) than in the rest of Japan (5.4/100,000).
Insulin resistance is closely associated with both aging and overweight; yet in old age, weight loss is common, although insulin resistance increases. To study this paradox, the authors evaluated the role of insulin resistance in weight change among older adults from the Rancho Bernardo Study cohort. Participants were 725 nondiabetic men and women who were aged 50–89 years when weight and insulin were measured at baseline (1984–1987). The participants were evaluated again in 1992–1996, at which time weight was remeasured. Fasting insulin and homeostasis model assessment (HOMA) measurements were evaluated in separate but parallel statistical models as surrogates for insulin resistance. Insulin resistance, when defined as the top quartile of fasting insulin level or HOMA value, was significantly associated with weight loss before and after adjustment for baseline weight and age (fasting insulin: ? = -1.30 kg, p = 0.01; HOMA: ? = -1.18 kg, p = 0.01). Results were the same for men versus women, for the overweight (body mass index (weight (kg)/height (m)2) 26.6) versus the normal weight (body mass index >26.6), and for younger persons (age <70 years) versus older persons (age 70 years). Insulin-resistant individuals had a threefold increased likelihood of losing 10 or more kg compared with those without insulin resistance. The authors conclude that hyperinsulinemia, independently of age and baseline weight, may have a catabolic effect in the elderly.
Tofu is a commonly consumed food in China. Tofu may interfere with lead absorption and retention because of its high calcium content. In this observational study, the authors examined whether dietary tofu intake was associated with blood lead levels among young adults in Shenyang, China. The analyses included 605 men and 550 women who completed baseline questionnaires and had blood lead measurements taken in 1996–1998 as part of a prospective cohort study on reproductive health. Mean blood lead levels were 13.2 µg/dl in men and 10.1 µg/dl in women. Blood lead levels were negatively associated with tofu intake in both genders. A linear trend test showed a 3.7% (0.5-µg/dl) decrease in blood lead level with each higher category of tofu intake (p = 0.003). The highest tofu intake group (750 g/week) had blood lead levels 11.3% lower (95% confidence interval: 4.1, 18.0) than those of the lowest tofu intake group (<250 g/week). In all regression models, data were adjusted for gender, age, height, body mass index, district, cigarette smoking, alcohol drinking, education, occupation, use of vitamin supplements, season, and dietary intake of meat, fish, vegetables, eggs, and milk. In conclusion, the authors found a significant inverse dose-response relation between tofu consumption and blood lead levels in this Chinese population.
Spatial statistical analysis of 1994–1995 small-area malaria incidence rates in the population of the northernmost districts of KwaZulu Natal, South Africa, was undertaken to identify factors that might explain very strong heterogeneity in the rates. In this paper, the authors describe a method of adjusting the regression analysis results for strong spatial correlation in the rates by using generalized linear mixed models and variograms. The results of the spatially adjusted, multiple regression analysis showed that malaria incidence was significantly positively associated with higher winter rainfall and a higher average maximum temperature and was significantly negatively associated with increasing distance from water bodies. The statistical model was used to produce a map of predicted malaria incidence in the area, taking into account local variation from the model prediction if this variation was supported by the data. The predictor variables showed that even small differences in climate can have very marked effects on the intensity of malaria transmission, even in areas subject to malaria control for many years. The results of this study have important implications for malaria control programs in the area.
In the past decade, there have been enormous advances in the use of Bayesian methodology for analysis of epidemiologic data, and there are now many practical advantages to the Bayesian approach. Bayesian models can easily accommodate unobserved variables such as an individual's true disease status in the presence of diagnostic error. The use of prior probability distributions represents a powerful mechanism for incorporating information from previous studies and for controlling confounding. Posterior probabilities can be used as easily interpretable alternatives to p values. Recent developments in Markov chain Monte Carlo methodology facilitate the implementation of Bayesian analyses of complex data sets containing missing observations and multidimensional outcomes. Tools are now available that allow epidemiologists to take advantage of this powerful approach to assessment of exposuredisease relations.
In epidemiologic studies, much of the variation in disease risk estimates associated with occupational pesticide exposure may be due to variation in exposure classification. The authors compared five different methods of using interview information to assess occupational pesticide exposure in a US-Canada case-control study of neuroblastoma (1992–1994). For each method, exposure assignment was compared with that of a reference method, and neuroblastoma effect estimates were calculated. Compared with the reference method, which included a complete review of occupation, industry, job tasks, and exposure-specific activities, the use of occupation-industry groups alone or in combination with general job task information diluted the exposed group by including individuals who were unlikely to have been truly exposed. The effect estimates representing associations between each exposure method and neuroblastoma were different enough to influence the study's conclusions, especially when the exposure was rare (for maternal occupational pesticide exposure, the odds ratio was 0.7 using the reference exposure assessment method and 3.2 using the occupation-industry group exposure assessment method). Exposure-specific questions about work activities can help investigators distinguish truly exposed individuals from those who report exposure but are unlikely to have been exposed above background levels and from those who have not been exposed but are misclassified as exposed because of their employment in an occupationindustry group determined a priori to be exposed.
The authors studied fracture risk among 16,416 Danish patients with bowel disease. All patients diagnosed with celiac disease (n = 1,021), Crohn’s disease (n = 7,072), or ulcerative colitis (n = 8,323) in Denmark between January 1, 1983, and December 31, 1996, were included. Each patient was compared with three age- and gender-matched controls randomly drawn from the background population. No increase in fracture risk could be demonstrated for celiac disease before or after diagnosis. In patients with Crohn’s disease, overall fracture risk was increased both before diagnosis (incidence rate ratio = 1.15, 95% confidence interval (CI): 1.00, 1.32) and after diagnosis (incidence rate ratio = 1.19, 95% CI: 1.06, 1.33). Bowel surgery was associated with a decreased risk of sustaining a fracture before diagnosis (odds ratio = 0.70, 95% CI: 0.54, 0.90) and after diagnosis (hazard ratio = 0.81, 95% CI: 0.67, 0.99). Overall fracture risk was not increased in patients with ulcerative colitis, except for a small increase around the time of diagnosis. Increasing age and having a fracture before diagnosis increased the risk of sustaining a new fracture after diagnosis. Crohn’s disease was associated with a minor increase in overall fracture risk in contrast to ulcerative colitis and celiac disease. The severity of the inflammatory process and the amount of corticosteroids given may explain the difference in fracture risk.
Low socioeconomic status is associated with high mortality, but the extent to which socioeconomic status affects particular diseases and whether socioeconomic status effects have changed over time are uncertain. The authors used education as a marker for socioeconomic status in a study of two large American Cancer Society cohorts (follow-up, 1959–1996). Low education was associated with higher death rates in both cohorts from all causes and most specific causes, except breast cancer and external causes among women. Life expectancy in the more recent cohort was 4.8 years shorter for men and 2.7 years shorter for women for the least versus the most educated. The inverse relation between education and mortality was strongest for coronary heart disease, lung cancer, diabetes, and chronic obstructive pulmonary disease; moderate for colorectal cancer, external causes (men only), and stroke; weak for prostate cancer; and reversed for external causes among women. The direction of a weak gradient for breast cancer differed for those with and without prevalent breast cancer at baseline. Adjustment for conventional risk factors, probable intermediate variables between education and mortality, diminished but did not eliminate the observed educational/mortality gradients. Temporal trends showed increasing mortality differences by education for coronary heart disease, diabetes, and lung cancer for women.
Several studies have suggested that high intake of fats and fat-rich foods may increase the risk of ovarian cancer. The authors examined these relations in the Nurses’ Health Study cohort. Dietary intake was assessed in 1980, 1984, 1986, and 1990 by using a self-administered food frequency questionnaire. Food data were used to calculate intake of various fats and fatty acids. For best reflection of long-term intake, an updated, cumulative, averaged measure of fat intake was used to predict incidence of ovarian cancer. Between 1980 and 1996, 301 incident cases of invasive epithelial ovarian cancer were confirmed among the 80,258 participants who completed the baseline food frequency questionnaire. There was no evidence of a positive association between intake of any type of fat and ovarian cancer risk, even after adjustment of fat subtypes for one another. Women in the highest quintile of total fat intake were not at increased risk compared with those in the lowest quintile (multivariate relative risk = 1.03, 95 percent confidence interval: 0.72, 1.45, p for trend = 0.97). Intakes of fat-rich foods were also not appreciably associated with ovarian cancer risk, although an increase in risk with frequent intake of eggs was observed. Overall, results suggest no association between intake of any type of fat and ovarian cancer.
Bell’s palsy is a relatively common disease characterized by the sudden onset of unilateral facial paralysis. Using a centralized surveillance system that contains demographic, military assignment, and medical encounter data of US military service members, the authors estimated rates, trends, and demographic correlates of risk of Bell’s palsy during a 2-year period. Poisson regression was used to estimate the independent effects of climate, season, and latitude. From October 1997 to September 1999, there were 1,181 incident cases of Bell’s palsy among US service members. The crude incidence rate was 42.77 per 100,000 personyears. Incidence rates increased with age and were higher among females, Blacks, Hispanics, married persons, and enlisted service members. Both climate (adjusted rate ratio for arid vs. nonarid climate = 1.34) and season (adjusted rate ratio for cold vs. warm months = 1.31) were independent predictors of risk of Bell’s palsy. Latitude was not a statistically significant predictor when demographic, climate, and season effects were taken into account. The results are consistent with hypotheses regarding viral etiologies (e.g., reactivation of herpes simplex) of Bell’s palsy.
Dengue viruses are a major cause of morbidity in tropical and subtropical regions of the world. Knowledge about the epidemiology and host determinants of inapparent and severe dengue virus infections is limited. In this paper, the authors report findings from the first 3 years of a prospective study of dengue virus transmission and disease severity conducted in a cohort of 2,119 elementary school children in northern Thailand. A total of 717,106 person-school days were observed from 1998 to 2000. The incidence of inapparent and of symptomatic dengue virus infection was 4.3% and 3.6% in 1998, 3.2% and 3.3% in 1999, and 1.4% and 0.8% in 2000, respectively. Symptomatic dengue virus infection was responsible for 3.2%, 7.1%, and 1.1% of acute-illness school absences in 1998, 1999, and 2000, respectively. The early symptom complex of acute dengue virus infection is protean and difficult to distinguish from other causes of febrile childhood illnesses. The authors’ results illustrate the spatial and temporal diversity of dengue virus infection and the burden of dengue disease in schoolchildren in Thailand. Their findings increase understanding of dengue virus transmission and disease severity in a well-defined cohort population and offer a study design in which to test the efficacy of potential dengue vaccines.
Dengue virus occurs as four distinct serotypes, each of which causes epidemics throughout the tropical and subtropical regions of the world. Few studies have examined co-circulation of multiple dengue virus serotypes in a well-defined cohort population over time and their capacity to produce severe dengue disease. In this paper, the authors report the details and findings of the first 3 years (1998–2000) of an ongoing prospective study of dengue virus transmission and disease severity in a cohort of children in northern Thailand. A total of 108 dengue virus isolates were obtained from 167 acute dengue virus infections; 23% were DEN1, 35% were DEN-2, 41% were DEN-3, and 1% were DEN-4. Despite the proximity of the schools, there was marked spatial and temporal clustering of transmission of each dengue serotype. Serotype-specific antibody levels prior to the dengue transmission season were not predictive of the incidence of dengue virus infections or the predominant serotype transmitted at individual schools. All dengue serotypes produced severe dengue illness, although DEN-3 produced more severe symptoms than the other dengue serotypes. The authors’ findings emphasize the complexity of dengue serotype-specific virus transmission and severe dengue disease and have important implications for dengue control and vaccine development.
The authors conducted a study examining perceived enabling factors and barriers to a successful career in epidemiology, the role of mentoring in facilitating one’s career, where graduates are most often being employed, and key competencies for future epidemiologic training. During June to August 2001, they surveyed senior epidemiologists across the United States (n = 248) in four sectors: state health departments, the Centers for Disease Control and Prevention, the National Institutes of Health, and schools of public health. The top enabling factors were dedication to hard work and having an intrinsic curiosity and a sense of discovery. The most frequently cited barrier was balancing career and family life, except among minority respondents, for whom an unsupportive supervisor was the leading obstacle. Influential characteristics of a mentor were high integrity and the provision of inspiration and encouragement. The top competencies anticipated for the next 10 years were skills working in multidisciplinary teams and in using modern information technologies. Important competencies varied somewhat according to work sector. These findings may be useful in training and career planning among aspiring epidemiologists and for educational policy development among organizations promoting training and mentoring.
In the Smokers and Nonsmokers Study, the authors investigated the feasibility of using random digit dialing telephone interviews to locate adults in the continental United States who were willing to provide DNA from buccal swabs through the mail. Interviews with 3,383 adults regarding their smoking-related behaviors (response rate = 70%) were conducted in 1999-2000; swab returns continued into early 2001. Overall, 57% of interviewees agreed to receive mailed information explaining the study. Better-educated persons (odds ratio (OR) = 1.3, 95% confidence interval (CI): 1.1, 1.6), younger persons (OR = 0.988, 95% CI: 0.983, 0.992), persons with symptoms of depression (OR = 1.8, 95% CI: 1.4, 2.4), and current smokers (OR = 2.25, 95% CI: 1.8, 2.8) were likelier to agree to receive a mailing. Approximately 26% of interviewees (45% of those receiving kits) returned buccal swabs, and 18% were successfully genotyped. Older (OR = 1.02, 95% CI: 1.01, 1.03), better-educated (OR = 1.4, 95% CI: 1.1, 1.7), and White (OR = 1.8, 95% CI: 1.4, 2.5) participants were more likely to return DNA samples, but current smokers (OR = 0.6, 95% CI: 0.5, 0.8) were less likely to do so. Participants were randomly assigned to one of two forms of participation: the "registry" group (names were kept on file) or the "made-anonymous" group (names were unassociated with samples). The two groups were equally likely to return kits, but registry respondents were more likely to nominate siblings for participation in the study (OR = 1.6, 95% CI: 1.2, 2.1). The participants in this study were similar demographically to the national population. The authors conclude that random digit dialing surveys coupled with mail collection of DNA may constitute a practical method of obtaining DNA samples for biobehavioral research.
The objective of this study was to assess agreement on nutrient intake between the nutrient database of the First National Health and Nutrition Examination Survey (NHANES I) and an upto-date (December 1998) nutrient database, the ESHA Food Processor. Analysis was conducted among 11,303 NHANES I participants aged 25–74 years in 1971–1975 who had undergone dietary assessment. A list of all unique foods consumed was obtained from a single 24-hour dietary recall questionnaire administered during the baseline NHANES I visit. Foods on the list were matched to foods in the ESHA Food Processor software. Agreement between participants’ nutrient intakes as calculated with the NHANES I and ESHA nutrient databases was assessed using intraclass correlation analysis, linear regression analysis, and graphic methods. Intraclass correlation analysis demonstrated excellent concordance between most nutrient intakes, with coefficients above 0.95 for intakes of energy, carbohydrates, protein, cholesterol, and calcium; coefficients between 0.90 and 0.95 for intakes of total fat, saturated fat, potassium, and vitamin C; and coefficients of approximately 0.85 for intakes of sodium and vitamin A. Graphic methods and regression analyses also showed good-to-excellent correspondence for most nutrients. These findings support the validity of expanding existing nutrient intake databases to explore current hypotheses, provided that food formulation, enrichment, and fortification practices have not qchanged substantially over time.
An assumption in case-control studies is that forces of selection are the same for cases and controls. This may not be true for studies of male infertility among infertility clients. Earlier reproductive outcomes may introduce modification of risk behavior or differential referral. Selection bias might also occur when infertile males are compared with fertile males. Partners of sterile men are more likely to have "normal" fertility, while partners of a reference group of normozoospermic men tend to have a lower fertility potential. The latter may lead to overrepresentation of causes of reduced female fertility and introduce bias into estimates of risk factors shared by couples. The relation between cigarette smoking and semen quality was studied in a population of infertility clients from the Netherlands during 1995–1996. To reduce the potential for bias, this relation was studied first in a restricted population less aware of the type of infertility involved. The odds ratio of infertility with smoking was elevated in the restricted population as compared with the total population. Adjustment for smoking by the female partner increased the odds ratios for male smoking as well. These results indicate that bias may occur in clinic-based fertility studies because of different forms of selection.
The widely used generalized additive models (GAM) method is a flexible and effective technique for conducting nonlinear regression analysis in time-series studies of the health effects of air pollution. When the data to which the GAM are being applied have two characteristics—1) the estimated regression coefficients are small and 2) there exist confounding factors that are modeled using at least two nonparametric smooth functions—the default settings in the gam function of the S-Plus software package (version 3.4) do not assure convergence of its iterative estimation procedure and can provide biased estimates of regression coefficients and standard errors. This phenomenon has occurred in time-series analyses of contemporary data on air pollution and mortality. To evaluate the impact of default implementation of the gam software on published analyses, the authors reanalyzed data from the National Morbidity, Mortality, and Air Pollution Study (NMMAPS) using three different methods: 1) Poisson regression with parametric nonlinear adjustments for confounding factors; 2) GAM with default convergence parameters; and 3) GAM with more stringent convergence parameters than the default settings. The authors found that pooled NMMAPS estimates were very similar under the first and third methods but were biased upward under the second method.
The authors examined the impact of potent antiretroviral therapy (ART) on the diagnosis of wasting syndrome in the Multicenter AIDS Cohort Study. Study time was divided into the periods 1988–1990, 1991–1993, 1994–1995, and 1996–1999 to correspond to different treatment eras. The proportion of acquired immunodeficiency syndrome diagnoses in which wasting was present increased from 5% in 1988–1990 to 7.1% in 1991–1993, 7.7% in 1994–1995, and 18.9% in 1996–1999. The incidence of wasting per 1,000 person-years increased from 7.5 in 1988–1990 to 14.4 in 1991–1993 and 22.1 in 1994–1995; it decreased to 13.4 in 1996–1999. Fewer patients with wasting had low hemoglobin and hematocrit levels and reported oral thrush in 1996–1999 than in any other period. Analysis of change in body mass index (weight (kg)/height (m)2) after wasting showed a faster return to prewasting levels in 1994–1995 and 1996–1999 than in earlier periods. Case-control analysis showed that wasting prior to 1996 was weakly associated with fatigue (p = 0.10), low hemoglobin (p = 0.11), and CD4-positive T-lymphocyte count (p = 0.04). During 1996–1999, wasting was weakly associated with diarrhea (p = 0.05) and potent ART (p = 0.097). Predictors of wasting have changed with potent ART. Further research is needed to determine whether lipodystrophy may be misdiagnosed as wasting syndrome.
Congress enacted the Americans with Disabilities Act (ADA) to provide persons living with the human immunodeficiency virus (HIV) and other vulnerable populations with legal means of redress against discrimination, yet virtually nothing is known about how the intended beneficiaries have used these protections. This study aimed to describe the epidemiology of ADA charges alleging employment-related discrimination due to HIV and to investigate the charge-filing behavior of workers with HIV. Using a national database of all HIV discrimination charges filed since the inception of the ADA in 1991, the author described respondent employers, issues in dispute, and outcomes of charges. Next, he used multivariate regression analyses to compare the sociodemographic characteristics of charge filers with those of a nationally representative baseline sample of workers with HIV. Of the 3,520 HIV discrimination charges filed through 1999, 18.0% had merit and 14.1% received monetary compensation. Workers who were female (odds ratio (OR) = 0.79, p < 0.01), aged less than 25 years (OR = 0.36, p < 0.01), and aged 25-34 years (OR = 0.77, p < 0.01) filed disproportionately fewer charges. Controlling for underlying rates of discrimination in the baseline population magnified this "underclaiming" among young workers. The findings should help to target dissemination and support activities, designed to help workers take advantage of antidiscrimination protections, at the subgroups of workers who need them most.
Drug resistance is contributing to increasing mortality from malaria worldwide. For assessment of the role of resistance-conferring parasite mutations on treatment responses to sulfadoxinepyrimethamine (SP) and transmission potential, 120 subjects with uncomplicated falciparum malaria from Buenaventura, Colombia, were treated with SP and followed for 21 days in the period February 1999 to May 2000. Exposures of interest were mutations in Plasmodium falciparum dihydrofolate reductase (DHFR) and dihydropteroate synthase that confer resistance to pyrimethamine and sulfadoxine, respectively. Although SP was highly efficacious (96.7%), the presence together of DHFR mutations at codons 108 and 51 was associated with longer parasite clearance time (relative hazard = 0.24, p = 0.019) more so than the 108 mutation alone (relative hazard = 0.45, p = 0.188). This association remained after controlling for potential confounders. Infections with these mutations were also associated with the presence of gametocytes, the sexual form of the parasite responsible for transmission, 14 and 21 days after treatment (p = 0.016 and p = 0.048, respectively). Higher gametocytemia is probably due to DHFR mutations prolonging parasite survival under drug pressure, resulting in longer parasite clearance time and allowing asexual parasites to differentiate into gametocytes. These results suggest that even when SP efficacy is high, DHFR mutations that are insufficient to cause therapeutic failure may nevertheless increase malaria transmission and promote the spread of drug resistance.
The associations between body weight, raised blood pressure, and mortality remain controversial. The authors examined these relations by considering all degrees of obesity in the Düsseldorf Obesity Mortality Study (1961–1994). Among 6,193 obese German patients aged 18–75 years and having a body mass index (BMI) of 25 kg/m2, 1,059 deaths were observed after a median follow-up of 14.8 years. The entire cohort was grouped into quartiles according to BMI (25–<32, 32–<36, 36–<40, 40 kg/m2) and systolic blood pressure (SBP) (<140, 140–<160, 160–<180, 180 mmHg). Cox proportional hazards analyses were performed to adjust for age. For women, the mortality risk curves for the four BMI groups in relation to SBP were flat without crossing, whereas the risk curve for moderately obese men (BMI 25–<32 kg/m2) crossed the risk curves for the higher BMI groups. In the group of patients with very high blood pressure (SBP 180 mmHg), moderately obese subjects (BMI 25–<32 kg/m2) had a higher mortality risk for men when compared with the BMI group 32– <36 kg/m2 (hazard ratio =1.62, 95% confidence interval: 1.0, 2.7) but not for women (hazard ratio = 0.71, 95% confidence interval: 0.4, 1.2). These findings support previous observations that the risk of death is lower for hypertensive men in high compared with low BMI groups.
This longitudinal study describes playground games of children progressing across their first year of schooling. Boys, in comparison with girls, played more games, especially chase and ball games, and played a greater variety of games. Also, the variety of boys’ games increased across the school year. Girls played more verbal games than boys. The study found that facility with games forecast boys’ social competence and both boys’ and girls’ adjustment to first grade. Children’s groups remained ethnically segregated across the school year. Results are discussed in terms of the role of games as an important developmental task during middle childhood.
The mechanisms of the decline in coronary heart disease mortality are not fully elucidated. In particular, little is known about the trends in severity of myocardial infarction, which may have contributed to the mortality decline. This study examines indicators of myocardial infarction severity including Killip class, electrocardiogram descriptors, and peak creatine kinase values in a population-based, myocardial infarction incidence cohort to test the hypothesis that the severity of myocardial infarction declined over time. Between 1983 and 1994, 1,295 incident cases of myocardial infarction (mean age, 67 (standard deviation, 6) years; 43% women) occurred in Olmsted County, Minnesota. The median time between the onset of symptoms and presentation was 1.9 (interquartile range, 3.9) hours and declined over time (p = 0.018), while the use of reperfusion therapy increased. Over time, the hemodynamic presentation of patients did not change appreciably, but the proportion of persons with ST-segment elevation declined as did the occurrence of Q waves and peak creatine kinase values. These secular trends, which were largely independent from the time to first electrocardiogram and reperfusion therapy, indicate a decline in the severity of myocardial infarction over time. Am J
Vulvar vestibulitis is characterized by superficial pain during intercourse. Exploratory studies have suggested that oral contraceptives (OCs) could be associated with occurrence of vulvar vestibulitis. This 1995–1998 case-control study in Québec, Canada, sought to reassess this association. Included were 138 women with vulvar vestibulitis whose symptoms had appeared in the previous 2 years and 309 age-matched controls who were consulting their physicians for reasons other than gynecologic problems or contraception. Cases and controls were interviewed to obtain a detailed history of OC use and information on potential confounding factors. Relative risks were estimated by using logistic regression. The authors found that 4 percent of cases had never used OCs compared with 17 percent of controls. The relative risk of vulvar vestibulitis was 6.6 (95 percent confidence interval: 2.5, 17.4) for ever users compared with never users. When OCs were first used before age 16 years, the relative risk of vulvar vestibulitis reached 9.3 (95 percent confidence interval: 3.2, 27.2) and increased with duration of OC use up to 2–4 years. The relative risk was higher when the pill used was of high progestogenic, high androgenic, and low estrogenic potency. The possibility that OC use may contribute to the occurrence of vulvar vestibulitis needs to be evaluated carefully. Am J Epidemiol 2002;156:254–61.
Endometrial cancer is associated with endogenous and exogenous estrogen excess. Some investigators have posited that electromagnetic fields may influence cancer risk through estrogenic hormonal mechanisms; however, there have been no studies reporting on electric blanket exposure in relation to endometrial cancer. The authors examined this possible association between endometrial cancer risk and electric blanket or mattress cover use as part of a population-based, case-control study. This analysis included incident endometrial cancer cases 40–79 years of age, interviewed during 1994 (n = 148; response rate, 87%) and identified from the Wisconsin tumor registry. Female controls of similar age were randomly selected from population lists (n = 659; response rate, 85%). Information regarding electric blanket and mattress cover use and endometrial cancer risk factors was obtained through structured telephone interviews approximately 1 year after diagnosis. After adjustment for age, body mass index, and postmenopausal hormone use, the risk of endometrial cancer was similar among ever users (odds ratio = 1.04, 95% confidence interval: 0.70, 1.55) and among current users (odds ratio = 0.87, 95% confidence interval: 0.49, 1.54) as compared with never users. Despite its small size and potential misclassification of exposure, this study provides evidence against an association between electric blanket or mattress cover use and endometrial cancer.
Feline malignant lymphoma occurs commonly in domestic cats and may serve as a model for nonHodgkin’s lymphoma in humans. Several studies have suggested that smoking may increase the risk of non-Hodgkin’s lymphoma. To evaluate whether exposure to household environmental tobacco smoke (ETS) may increase the risk of feline malignant lymphoma, the authors conducted a casecontrol study of this relation in 80 cats with malignant lymphoma and 114 controls with renal disease diagnosed at a large Massachusetts veterinary teaching hospital between 1993 and 2000. Owners of all subjects were sent a questionnaire inquiring about the level of smoking in the household 2 years prior to diagnosis. After adjustment for age and other factors, the relative risk of malignant lymphoma for cats with any household ETS exposure was 2.4 (95 percent confidence interval: 1.2, 4.5). Risk increased with both duration and quantity of exposure, with evidence of a linear trend. Cats with 5 or more years of ETS exposure had a relative risk of 3.2 (95 percent confidence interval: 1.5, 6.9; p for trend = 0.003) compared with those in nonsmoking households. These findings suggest that passive smoking may increase the risk of malignant lymphoma in cats and that further study of this relation in humans is warranted.
Oxidation of biomolecules may play a role in susceptibility to a number of diseases. However, there are few large-scale survey data describing oxidative damage that occurs in humans and the demographic, physical, or nutritional factors that may be associated with it. Such information is essential for the design and analysis of studies investigating the role of oxidative stress in health and disease. This paper presents data on levels of two biomarkers of lipid peroxidation, malondialdehyde and F2-isoprostanes, in 298 healthy adults aged 19–78 years. The study was conducted in Berkeley and Oakland, California, in 1998–1999. Sex was the strongest predictor of lipid peroxidation as measured by both biomarkers (p < 0.0001); it was stronger than smoking. Creactive protein was positively associated with lipid peroxidation (p = 0.004), as was plasma cholesterol. Plasma ascorbic acid had a strong inverse relation (p < 0.001) with both biomarkers. Plasma ?-carotene was also associated with F2-isoprostanes. Other plasma antioxidants were not associated with lipid peroxidation biomarkers, once ascorbic acid was included in the multivariate model. Future surveys and epidemiologic studies should measure at least one marker of oxidative damage, as well as plasma ascorbic acid. These data would permit a better understanding of the role that oxidants and antioxidants play in the health of human populations.
With the expanded use of computerized databases to gather information, a concomitant interest in using databases for public health purposes has developed. The authors investigated correlates of consenting to participate in such databases. The Regional Perinatal Data System combines electronic birth certificate information with questions asked of all women delivering a livebirth. Each woman is asked to consent to share information with 1) her obstetric provider, 2) her infant’s pediatric provider, and 3) an immunization registry. From 1996 to 1999, women who responded to the consent question and whose livebirth did not result in death or adoption were included. Odds ratios with 95% confidence intervals denoted the magnitude of association for refusing consent. Women who were "self-pay" (odds ratio = 2.0, 95% confidence interval: 1.7, 2.4), foreign born (odds ratio = 1.9, 95% confidence interval: 1.7, 2.1), and aged 40 or more years (odds ratio = 2.0, 95% confidence interval: 1.6, 2.3) were more likely to refuse to share data. Women eligible for but not participating in the Special Supplemental Nutrition Program for Women, Infants, and Children were significantly more likely to not share their information with others (odds ratio = 1.5, 95% confidence interval: 1.3, 1.6), after controlling for confounders. Refusing to share information with other sources is not random, and women refusing consent often do not participate in publicly available programs.
The recent completion of the first draft of the human genome sequence and advances in technologies for genomic analysis are generating tremendous opportunities for epidemiologic studies to evaluate the role of genetic variants in human disease. Many methodological issues apply to the investigation of variation in the frequency of allelic variants of human genes, of the possibility that these influence disease risk, and of assessment of the magnitude of the associated risk. Based on a Human Genome Epidemiology workshop, a checklist for reporting and appraising studies of genotype prevalence and studies of gene-disease associations was developed. This focuses on selection of study subjects, analytic validity of genotyping, population stratification, and statistical issues. Use of the checklist should facilitate the integration of evidence from these studies. The relation between the checklist and grading schemes that have been proposed for the evaluation of observational studies is discussed. Although the limitations of grading schemes are recognized, a robust approach is proposed. Other issues in the synthesis of evidence that are particularly relevant to studies of genotype prevalence and gene-disease association are discussed, notably identification of studies, publication bias, criteria for causal inference, and the appropriateness of quantitative synthesis.
Growing knowledge about gene-disease associations will lead to new opportunities for genetic testing. Many experts predict that genetic testing will become increasingly important as a guide to prevention, clinical management, and drug treatment based on genetic susceptibilities. As part of a Human Genetic Epidemiology workshop convened by the Centers for Disease Control and Prevention, a group of experts evaluated the evidence needed when considering the appropriate use of new genetic tests. Because new tests are likely to vary in their predictive value, their potential to direct prevention or treatment efforts, and their personal and social consequences, the task of determining appropriate use will require careful consideration of a variety of factors, including the analytic validity, clinical validity, clinical utility, and ethical, legal, and social implications of the test. Standardized formats are needed to summarize what is known and not known about new genetic tests with respect to each of these features. Following criteria for the objective assessment of test properties, reports should be structured to enable policy makers, clinicians, and the public to identify the available evidence, so that uncertainties can be taken into account when considering test use and planning future research.